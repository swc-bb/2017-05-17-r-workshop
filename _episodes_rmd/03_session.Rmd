---
title: Code optimization (Vectorization, profiling, parallelization)
teaching: 30
exercises: 60
questions:
- "How can I speed-up my R-scripts?"
objectives:
- "Identify bottle necks using profiling"
- "Code optimization"
- "Parallelization"
keypoints:
- "Be fast, not furious."
source: Rmd
---


Execution times of scripts can be a limiting factor.
This lecture presents some strategies for code speed-up,
some of them general, some of them specific to R.

Let's define some computationally demanding task we'll use for benchmarking:
Remove all rows from a large table, where any measurement (in the columns) exceeds the threshold ```meas_thresh```

Here's some function to create some synthetic data for testing:

```{r,eval=FALSE}
#function to randomly create some data table 
  create_data <- function(rows2create) {
    #create a table with [rows2create] samples, each sample has been measured 10 times
    set.seed(1) #initialize random generator for reproducible randomness  
    much_data <- data.frame(cbind(
                        sampleID = paste0("sample",1:rows2create),
                           data.frame(matrix(runif(rows2create*10), nrow = rows2create))
                     ), stringsAsFactors = FALSE)
    names(much_data) <- c("sampleID", paste0("meas_",1:10)) #set column names
    return(much_data)
  }

```

OK, now use this function to create some 20000 rows of data:

```{r, eval = FALSE}
rows_in_testdata <- 20000
much_data <- create_data(rows2create = rows_in_testdata) #create table with 20 k rows

summary(much_data)

meas_thresh <- 0.8 #rows with any value higher than this shall be discarded
```

A very naive and straightforward way to address the task is going through the data row-by-row and checking each column against the threshold:

```{r, eval = FALSE}
#first attempt: naive nested loop
  clean_data_1 <- function ()
  {
    cleaned_data<-data.frame()
    for (rr in 1:nrow(much_data))
    {
      valid_sample <- TRUE #initialise flag       
      for (cc in 2:ncol(much_data))
      {
        if (much_data[rr, cc] > meas_thresh)
          valid_sample <- FALSE
      }
      if (valid_sample)
        cleaned_data <- rbind(cleaned_data, much_data[rr,])
    }
    return(cleaned_data)  
  }
```

Apply the function:

```{r, eval = FALSE}
# test the function
  cleaned_data1 <- clean_data_1()
```

Yikes, that took long. How long exactly?

> ## Challenge 1
>
> Find out how long (in seconds) the function runs.
>
> Hint: ```Sys.time()``` gives the current time.
>
> > ## Solution to challenge 1
> >
> > ```{r, eval = FALSE}
> > start <- Sys.time()
> > cleaned_data1 <- clean_data_1()
> > stop  <- Sys.time()
> > stop-start 
> > ```
> {: .solution}
{: .challenge}
So this took 14.2 s.
(These times will vary and depend on your machine. I'll report my results here henceforward for illustration nevertheless.)

We can do this slightly more elegant:    
```{r, eval = FALSE}
system.time(clean_data_1())
```
The output means:
User: time needed for the actual task
System: time additionally needed for system calls
elapsed: total time elapsed

###Profiling
So what takes so much time? Like in a thriller, the villain may be found with the help of a profiler.
Here, a profiler gives us detailed information on the required system resources (CPU and RAM).
Let's do some "profiling"...
```{r, eval = FALSE}
  Rprof("prof.out") #start a profiling session and put the output to a file "prof.out"
    cleaned_data1 <- clean_data_1()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")
```
The output means:
by.self: time spent in function alone.
by.total: time spent in function and callees.
sample.interval: the sampling interval, by default every 20 msecs.
sampling.time: total time of profiling run. Remember that profiling does impose a small performance penalty.
  
We can spice this up using some graphics:
```{r, eval = FALSE}
# graphical profiling with profvis (also implemented in RStudio GUI)  
  if (!require(profvis))
  {
    install.packages("profvis", dependencies = TRUE)
    library(profvis)
  }
  profvis(prof_input = "prof.out") #we are using the existing file
```  

Alternatively, mark a section in RStudio and click "Profile" <- "Profile selected lines".

There are other, even fancier options for displaying the results. If you get bored during this lecture, have a peek at them. If you find out what all those numbers mean, let me know.
```{r, eval = FALSE}
#graphical profiling with proftools (demanding dependencies, fancy node plot)
if (!require(proftools))
{
  install.packages("proftools", dependencies = TRUE)
  library(proftools)
}

if (!require(Rgraphviz) | !require(graph))
{
  source("https://bioconductor.org/biocLite.R")
  biocLite("graph")
  biocLite("Rgraphviz")
  library(graph)
  library(Rgraphviz)
}

#pdf("myprofile.pdf") #optionally put output into PDF
plotProfileCallGraph(readProfileData("prof.out"), score = "total")
#dev.off() #close PDF. Now you can open the PDF and zoom in to read the smaller numbers.
```

###Code optimization
Apparently, ```rbind()``` consumes most time, as it allocates a new data frame every time.

> ## Challenge 2
>
> Do pre-allocation of ```cleaned_data``` and enter the valid data into it using a counter
> > ## Solution to challenge 2
> >
 ```{r ch2-sol}
 clean_data_2 <- function ()
  {
    cleaned_data <- much_data #pre-allocate: create an data frame with the same structure
    valid_rows <- 0
    
    for (rr in 1:nrow(much_data))
    {
      valid_sample <- TRUE #initialise flag       
      for (cc in 2:ncol(much_data))
      {
        if (much_data[rr, cc] > meas_thresh)
          valid_sample <- FALSE
      }
      if (valid_sample)
      {  
        valid_rows <- valid_rows + 1 #increase counter
        cleaned_data[valid_rows,] <- much_data[rr,]
      }  
    }
    return(cleaned_data[1:valid_rows,])  
  }
 ```
> {: .solution}
{: .challenge}

Let's check if that helped:
```{r, eval = FALSE}
Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data2 <- clean_data_2()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")
  range(cleaned_data1[,-1] - cleaned_data2[,-1]) #check the result is still the same
  rm(cleaned_data1) #remove the old result, we don't want to clutter all our workspace
```  
10.8 s. Well done, runtime significantly reduced.

> ## Tip: Pre-allocating memory is generally a good idea.
{: .callout}

The output suggests, that the dataframe operations consume much time. While dataframes and lists are convenient, computations on them are slower than on ordinary matrices.

> ## Challenge 3
>
> In the function, convert the dataframe to matrix and work with the matrix only (let's discard the first column).
>
> > ## Solution to challenge 3
> >
> > ```{r, eval = FALSE}
> >  clean_data_3 <- function ()
> >  {
> >    much_data_matrix <- as.matrix(much_data[,-1])
> >    cleaned_data <- array(0, dim = dim(much_data_matrix)) #pre-allocate: create an empty data matrix
> >    valid_rows <- 0
> >    
> >    for (rr in 1:nrow(much_data))
> >    {
> >      valid_sample <- TRUE #initialise flag       
> >      for (cc in 1:ncol(much_data_matrix))
> >      {
> >        if (much_data_matrix[rr, cc] > meas_thresh)
> >          valid_sample <- FALSE
> >      }
> >      if (valid_sample)
> >      {  
> >        valid_rows <- valid_rows + 1 #increase counter
> >        cleaned_data[valid_rows,] <- much_data_matrix[rr,]
> >      }  
> >    }
> >    return(cleaned_data[1:valid_rows,])  
> >  }
> > ```
> {: .solution}
{: .challenge}

Check it out:
```{r, eval = FALSE}
  Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data3 <- clean_data_3()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")

  range(cleaned_data2[,-1] - cleaned_data3) #check the result is still the same
  rm(cleaned_data2) #remove the old result
```
0.35 s! Wow, that is some speed-up! Though, admittedly, we sacrificed the first column containing the string 
> ## Tip: avoid complex data structures, especially when containing different datatypes (strings, numbers, factors, ...)
{: .callout}

We'll increase the size of the data so we can still appreciate further changes:
```{r, eval = FALSE}  
  much_data <- create_data(rows2create = rows_in_testdata*10) #create table with 200 k rows

  Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data3 <- clean_data_3()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")
```
3.1 s. Let this be our new benchmark.

The comparison and the repeated determination of the number of columns still takes some time.

> ## Challenge 4
>
> Shortcut comparisons, put static computations (```nrow()```) outside loop!
>
> > ##  Challenge 4
> >
> > ```{r, eval = FALSE}
> >  clean_data_4 <- function ()
> >  {
> >    much_data_matrix <- as.matrix(much_data[,-1])
> >    cleaned_data <- array(0, dim = dim(much_data_matrix)) #pre-allocate: create an empty data matrix
> >    valid_rows <- 0
> >    cols <- ncol(much_data_matrix)
> >    for (rr in 1:nrow(much_data))
> >    {
> >      valid_sample <- TRUE #initialise flag       
> >    
> >      for (cc in 1:cols)
> >      {
> >        if (much_data_matrix[rr, cc] > meas_thresh)
> >        {
> >          valid_sample <- FALSE
> >          break
> >        }  
> >      }
> >      if (valid_sample)
> >      {  
> >        valid_rows <- valid_rows + 1 #increase counter
> >        cleaned_data[valid_rows,] <- much_data_matrix[rr,]
> >      }  
> >    }
> >    return(cleaned_data[1:valid_rows,])  
> >  }
> > ```
> {: .solution}
{: .challenge}

Now have a go:
```{r, eval = FALSE}  
  Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data4 <- clean_data_4()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")

  range(cleaned_data3 - cleaned_data4) #check the result is still the same
  rm(cleaned_data3) #remove the old result
```
1.8 s. Again, runtime reduced by roughly one third.

> ## Tip: Place static computations outside loop, use shortcuts.
{: .callout}


The comparison still takes some time.

> ## Challenge 5
>
> Don't criticize it, vectorize it: The vectorisation of the inner loop
> Instead of using the loop, compare an entire row to threshold and use ```any()```   
> > ##  Challenge 5
> >
> > ```{r, eval = FALSE}
> >  clean_data_5 <- function ()
> >  {
> >    much_data_matrix <- as.matrix(much_data[,-1])
> >    cleaned_data <- array(0, dim = dim(much_data_matrix)) #pre-allocate: create an empty data matrix
> >    valid_rows <- 0
> >    cols <- ncol(much_data_matrix)
> >    for (rr in 1:nrow(much_data))
> >    {
> >      valid_sample <-  !any(much_data_matrix[rr, ] > meas_thresh)
> >        
> >      if (valid_sample)
> >      {  
> >        valid_rows <- valid_rows + 1 #increase counter
> >        cleaned_data[valid_rows,] <- much_data_matrix[rr,]
> >      }  
> >    }
> >    return(cleaned_data[1:valid_rows,])  
> >  }
> > ```
> {: .solution}
{: .challenge}

Test it:
```{r, eval = FALSE}  
Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data5 <- clean_data_5()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")

  range(cleaned_data4 - cleaned_data5) #check the result is still the same
  rm(cleaned_data4) #remove the old result
```
0.95 s! Runtime cut in half again! 
> ## Tip: Vector expressions beat loops.
{: .callout}


Can we beat that?

Let's be ambitious and increase the size of the data even more by factor 5.
Remember: by now we are dealing with 50 times the amount of data we started with!
```{r, eval = FALSE}  
   much_data <- create_data(rows2create = rows_in_testdata*10*5) #create table with 20*10*5 k rows

   Rprof("prof.out") #start a profiling session and put the output to "prof.out"
   cleaned_data5 <- clean_data_5()
   Rprof(NULL) #stop the profiling session
   summaryRprof(filename = "prof.out")
```   
2.8 s. The new benchmark.

So, vectorisation proved very effective.

> ## Challenge 6
>
> Vectorize the outer loop using ```apply()``` and an internal function that evaluates the validity of an entire row
> > ##  Challenge 6
> >
> > ```{r, eval = FALSE}
> >  clean_data_6 <- function ()
> >  {
> >    much_data_matrix <- as.matrix(much_data[,-1])
> >    #cleaned_data <- array(0, dim = dim(much_data_matrix)) #pre-allocate: create an empty data matrix
> >    
> >    validate_row <- function(row2test) #in a given row, test if all values are below threshold
> >    {
> >      return(! any(row2test > meas_thresh))
> >    }
> >    
> >    valid_rows <- apply(X = much_data_matrix, MARGIN = 1, FUN = validate_row)
> >    
> >    return(much_data_matrix[valid_rows,])  
> >  }
> > ```
> {: .solution}
{: .challenge}
  
Let it roll:
```{r, eval = FALSE}  
Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data6 <- clean_data_6()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")
  
  range(cleaned_data5 - cleaned_data6) #check the result is still the same
  rm(cleaned_data5) #remove the old result
```  
2.4 s. We did it again, although not that impressively.  

> ## Tip: ```apply()``` can be an efficient replacement for loops.
{: .callout}


With 50 times the data we reduced runtime from 18.9 to 2.4 s
This is 390-fold speedup!

What else can we do?
DLL (Dynamically linked libraries): Let someone else do the work!

*** Calling external functions

R is an interpreter language. As such, it is convenient to use, but necessarily 
less computationally efficient. 
In turn, compiler languages are more cumbersome to apply, but faster.
Among them, Fortran is THE traditional number cruncher.

Have a look at the file ```fortran/external_library.f90``` . No need to understand all.
This file needs to be compiled.
When compiling, make sure the architecture (32 / 64 bit) is consistent between R and compiler.
From the console, type:
  ```gfortran --shared -o external_library.dll external_library.f90```
alternatively, let ```R``` handle the job:
  ```R CMD SHLIB external_library.f90```
(on Windows, this requires ```RTools``` to be installed)

Now, let's first just try "maxi", an external realization of the max() function:

```{r, eval = FALSE}  
dyn.load("external_library.dll") #load the external library
  if (!is.loaded("maxi")) stop("Couldn't load DLL or find function") else  #check if has been loaded correctly and the function we want is part of it
    print("DLL loaded, function found")
```  
The external function must be called with precisely the same syntax.
The data types of R and compiler code must match exactly.
The array size must be passed as a separate parameters (no dynamic array size, sorry).
```{r, eval = FALSE}    
  testvector <- runif(10)
  out <- .Fortran("maxi", 
                 vect     = as.double(testvector ), 
                 vectlen  = as.integer(length(testvector)), 
                 mm       = as.double(1) 
                 )
  str(out) #the result is returned as a list, also containing all given arguments
  out$mm == max(testvector)  #the result is the same as from the internal function
```  
Now for the real stuff: let's use ```clean_data_7```, which addresses our actual task: 

```{r, eval = FALSE}  

if (!is.loaded("clean_data_7")) stop("Couldn't load DLL or find function") else  #check if has been loaded correctly and the function we want is part of it
    print("DLL loaded, function found")

```  

Unfortunately, the external function cannot return the already cleaned matrix, because dynamically sizing the the return value is not possible (afaik).
We have to do the actual selection of the valid rows in R.

> ## Challenge 7
>
> Put the external function into some wrapper function to achieve this.
> > ##  Challenge 7
> >
> > ```{r, eval = FALSE}
> >  clean_data_7 <- function()
> >  {  
> >    much_data_matrix <- as.matrix(much_data[,-1])
> >    out <- .Fortran("clean_data_7", 
> >                   datamatrix = as.double(much_data_matrix ), 
> >                   rows       = as.integer(NROW(much_data_matrix)), 
> >                   cols       = as.integer(NCOL(much_data_matrix)), 
> >                   meas_thresh= as.double(meas_thresh), 
> >                   valid_rows = as.logical(rep(0,NROW(much_data_matrix))))
> >    return(much_data_matrix[out$valid_rows,])  
> >  }
> >  cleaned_data7 <- clean_data_7()
> > ```
> {: .solution}
{: .challenge}

So, how does this perform?
```{r, eval = FALSE}  
  Rprof("prof.out") #start a profiling session and put the output to "prof.out"
  cleaned_data7 <- clean_data_7()
  Rprof(NULL) #stop the profiling session
  summaryRprof(filename = "prof.out")
  
  range(cleaned_data6 - cleaned_data7) #check the result is still the same
  rm(cleaned_data6) #remove the old result
  
  dyn.unload("external_library.dll") #unload the external library, when we don't need it any longer
```  

0.2 s. Totally great. That one will be hard to beat.

> ## Tip: Externally compiled code can be way faster than R.
{: .callout}


*** Parallelization: Divide et impera. 

R does not use multiple cores per se. However, a variety of approaches exist that allow parallelization.
They differ significantly in portability, ease-of-use and flexibility. Many comprise a front end (for controlling the tasks) and a back end (sometime platform-specific).
```foreach()``` is a very flexible front end:

```{r, eval = FALSE}  
if (!require(foreach))  #the parallel frontend, also works in serial mode
  {
    install.packages("foreach", dependencies = TRUE)
    library(foreach)
  }
``` 
It can be used serially:
```{r, eval = FALSE}  
  # serial use of foreach
b results <- foreach(i=1:10, .combine=rbind) %do% 
  {
    i^2
  }
  results
```
or in parallel:
```{r, eval = FALSE}  
  # parallel use of foreach
  results <- foreach(i=1:10, .combine=rbind) %dopar% 
  {
    i^2
  } 
``` 
The warning suggests, we still need to register a backend for this to really work in parallel.
As stated before, multiple backends exist. Let's stick to ```doParallel```:
```{r, eval = FALSE}  
#backends

  # #back-end for Linux only
  # if (!require(doMC)) 
  # {
  #   install.packages("doMC", dependencies = TRUE)
  #   library(doMC)
  # }
  # registerDoMC(2)  #change the 2 to your number of CPU cores  
  

  # backend for Windows + Linux
  # if (!require(doSNOW)) 
  # {
  #   install.packages("doSNOW", dependencies = TRUE)
  #   library(doSNOW)
  # }
  # cl<-makeCluster(4) #change the 4 to your number of CPU cores
  # registerDoSNOW(cl) 
  
  # backend for Windows + Linux
  if (!require(doParallel))
  {
    install.packages("doParallel", dependencies = TRUE)
    library(doParallel)
  }
  cl <- makePSOCKcluster(4) #initiate 4 nodes for computation. If you have more CPUs, increase this value.
  registerDoParallel(cl)
```

Let's make up something computationally intensive:
```{r, eval = FALSE} 
   max_eig <- function(n, size=500) { 
    set.seed(n)     
    d <- matrix(rnorm(size**2, sd = n), nrow = size)
    E <- eigen(d)$values
    return(abs(E)[[1]])
  }
```

> ## Challenge 8
>
> Compare serial and parallel execution times 
> (detailed profiling does no longer work here, use ```system.time()```)
> > ##  Challenge 8
> >
> > ```{r, eval = FALSE}
> >  #serial
> >  system.time(
> >    results <- foreach(i=1:10, .combine=rbind) %do% 
> >    {
> >      max_eig(n = i)
> >    }
> >  )  
> > ```
> >  14 s.
> > ```{r, eval = FALSE}
> >  # parallel 
> >  # (if you are quick, you will see multiple R-instances in the process list ("top" in Linux, Taskmanager in Windows))
> >  system.time(
> >    results <- foreach(i=1:10, .combine=rbind) %dopar% 
> >    {
> >      max_eig(n = i)
> >    }
> >  )  
> > ```
> > 5 s.
> {: .solution}
{: .challenge}
  
Not bad, but also not quite the speed-up one could expect when going from 1 to 4 nodes.
> ## Tip: Parallelization always comes with some overhead.
{: .callout}

What about our original task?
```{r, eval = FALSE}
  system.time(
    {  
    much_data_matrix <- as.matrix(much_data[,-1])
    valid_rows <- foreach(rr=1:nrow(much_data),.combine=rbind) %dopar% 
    {
      !any(much_data_matrix[rr, ] > meas_thresh)
    }
    }
  )
```
Dead slow! Too much data to be transfered, single threads are too small, the overhead eats up any potential benefits. Nothing to gain here.
  
```{r, eval = FALSE}  
  stopCluster(cl) #close cluster after use
```
 
> ## Tip: # Parallelization does not help for all kinds of tasks
  # Rules of thumb:
  # few data, few/no interaction, heavy computations          -<- parallelize
  # much data, interaction between threads, trivial computations -<-  don't parallelize (at least not that simply)
{: .callout} 
  
*** Misc 
What else can we do for speed-up?  
- disable screen output and graphics when possible
- check compiling functions, package ```Rcpp```
- keep an eye on memory use (or do memory profiling): 
 large memory requirements (e.g. by parallelization!) may require swapping RAM to hard disk, which is painfully slow
- avoid file operations
- if you have to access files, do it
 - in large chunks
 - in single files
 - preferably in binary ( ```save()``` ) opposed to text files (```read.table()``` )
  

*** Summary
Execution time changes drastically with some modifications. This usually helps:
 - preallocate memory
 - vectorize
 - avoid loops, use ```apply()```
 - use compiler languages for heavy tasks
 - consider parallelization, if the problem allows it  
  
  
**** pros
 - well, it's faster
**** cons
 - readability may deteriorate
 - transferability may decrease (especially with parallelization)
 - priorities and effort: spending days of (re-)coding for seconds of speed-up
   
  punchy summary of code optimization:
[readthis]: https://www.r-bloggers.com/faster-higher-stonger-a-guide-to-speeding-up-r-code-for-busy-people/

    

  

  
